
\section{Existing Models: Double-DOP and DOP$^*$}\label{sec:Existing}
%A description and comparison of Double-DOP and DOP*

%Iets met Goodman? Die behoudt alleen kleine stukjes en speciale regels, maar daardoor geen expliciete representatie van 'productive units' (Sangati, einde van sectie 2)

In this section, we outline two approaches to constrain the extraction of fragments: \ddop{} and \dops{}. Furthermore, we discuss the similarities and dissimilarities for these two approaches. 

\subsection{\ddop}
In the following, we discuss \ddop{} as it was presented in \cite{sangati2011}. In this model, no unique fragments are extracted from the dataset: if a construction occurs in one tree only, it is probably not representative for the language. This is carried out by a dynamic programming algorithm using tree-kernels. It iterates over pairs of trees in the treebank, looking for fragments they have in common. In fact, only the largest shared fragment is stored. 

The symbolic grammar that is the output of this algorithm is not guaranteed to derive each tree in the training corpus. Therefore all fragments of depth one, constituting the set of PCFG-productions, are also added.

The emphasis of \ddop{} is on the extraction method for determining the symbolic grammar. However, it was also implemented with different estimators. The estimation is done in a second pass over the treebank, gathering frequency counts for the fragments in the grammar. We will use the relative frequency estimate, which was empirically found to perform best \cite{sangati2011}.


%The best maximizing objective appears to be MCP (max constituent parse), which maximizes a weighted average of expected labeled recall and precision. The latter cannot be maximized directly, but is approximated by minimizing the mistake rate. 
%Parameter $\lambda$ that rules the linear interpolation was empirically found to be optimal at 1.15. Efficient calculation of MPC is possible with dynamic programming, but it can also be approximated with a list of $k$-best derivations.
%In addition, empirical results show that the relative frequency estimate outperforms the other estimates tested, i.e. Equal Weights Estimate (first adjust counts proportional to the size of the symbolic grammar, then make this value proportional to fragments with the same root), and Maximum Likelihood (optimizing to maximum likelihood for the training data with an Inside-Outside algorithm). 
%NB in contrast to earlier findings
%NB: unknown words approach: rare words, frequency <5, get a general tag based on morphological features

\subsection{\dops}
In \dops{} \cite{zollmann2005}, a rather different approach is taken called held-out estimation. The treebank is split in two parts, the \emph{extraction corpus}~($EC$) and a \emph{held-out corpus}~($HC$). An initial set of fragments is extracted from the $EC$, containing all the fragments from its trees. The weights are then determined so as to to maximize the likelihood of $HC$, under the assumption that this is equivalent to maximizing the joint probability of the \emph{shortest derivations} of the trees in $HC$. 

The weight of a fragment is its relative frequency of occurring in a shortest derivation, and all fragments that do not occur in such a derivation are removed from the symbolic grammar. In fact, a tree could have several shortest derivations. The probability mass is divided over the fragments taking parts in the different derivations in that case. Furthermore, some trees in $HC$ may not be derivable at all, which indicates that the grammar does not have complete coverage: a sentence in the test set might also be underivable. 

To maximize coverage of the grammar, \dops{} comes with a smoothing method. The relative frequency of underivable trees in $HC$ is denoted by $p_{unkn}$. This value is discounted from all the fragments in the grammar, and distributed over all the depth one fragments in the entire treebank ($HC\cup EC$).


\paragraph{Consistency and bias}
\dops{} was introduced as the first consistent (non-trivial) DOP-estimator. Zollmann provides a consistency proof in \cite{zollmann2005}. On the other hand, \dops{} is  biased, but Zollmann shows how bias actually arises from generalization: no non-overfitting DOP estimator could be unbiased. Bias is therefore not problematic but a desirable property of an estimator.

The consistency of \dops{} is fundamentally tied to the extraction of fragments in the shortest derivations. As the treebank size increases, the expected loss of the estimate will diminish, as described above. One of the goals of this project is to describe what influence this has on the distribution of weight over the fragments in the grammar.

In \cite{zuidema2006} it is argued that there is a problem with the consistency proof given for \dops{}, as well as the non-consistency proof for other DOP-estimators by \cite{johnson2002}. Zuidema points out that these proofs use a frequency-distribution test, whereas for DOP a weight-distribution test would be more appropriate. 
%Wat hiermee?






















