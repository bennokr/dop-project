
\section{Introduction}
%Introduction to the paper
%General introduction to the DOP framework, terminology 
%NB: introduce term 'fragment' for usage throughout this document



%Je hebt de introductie en terminologie helemaal uit elkaar getrokken. Wat op zich wel helder is, maar niet zo compact.. Aangezien we maar 9 pagina's mogen schrijven kunnen we dat denk ik beter samenvoegen.

\begin{figure}[h!]
\center \input{figureTreebank}
\caption{A toy treebank} \label{f:treebank}
\end{figure}

A common approach to natural language syntax, is to view the structure of sentences as constituent trees. An artificial example of a treebank is given in figure \ref{f:treebank}. Constituent trees can be described by a \emph{Context Free Grammars} (CFG),  such that all trees are built up from  rules that each describe the production (children nodes) of a single node (parent) in the tree. When building an empirical model of observed parse trees, these rules are extended with probabilities to form a \emph{probabilistic~CFG}~(PCFG). This gives the trees that are `generated' by these rules their own probability, which makes it a statistical model of a distribution over natural language syntax.

A CFG models each production as an independent event, but natural language probably has more complex interdependencies. To this end, grammars can be enriched (e.g. by Markovisation, that include information about grandparent or sibling nodes.


\subsection{DOP}
\emph{Data-Oriented Parsing} (DOP), as first introduced in \cite{scha1990}, takes a different approach. It models the language with a \emph{Probabilistic Tree Substitution Grammar}~(PTSG). 
The trees in the treebank are taken apart, which results in \emph{fragments} of arbitrary depth\footnote{Fragments are sometimes referred to as `subtrees' in literature}.
A fragment is a connected subgraph of a tree such that it corresponds to context-free productions in that tree, i.e. each node must have either have children with the same labels as in the original tree, or no children at all. This is illustrated in figure \ref{f:fragments}. Note that not all fragments from the treebank in figure \ref{f:treebank} are displayed. We see that each level-one fragment corresponds to a CFG rule. The \emph{symbolic grammar} refers to the set of fragments (that receive a non-zero weight) in a grammar. 



\begin{figure}[h!]
\center \input{figureFragments}
\caption{Extracted fragments}
\label{f:fragments}
\end{figure}


Fragments can be combined in a \emph{derivation} to build syntactic structures. A step in a derivation is a composition, denoted by the symbol $\circ$. For instance, tree $t_1$ can be derived as $t_1=f_2\circ f_{10}$. We follow the convention to only allow left-most derivations. This means that the left-most non-terminal node in a fragment $f_1$ must correspond to the root node of $f_2$ in order to derive $f_3=f_1\circ f_2$

A weight must be assigned to each fragment. This can be done by counting how often it occurs in the treebank compared to others with the same root, yielding the \emph{relative frequency estimate}. The probability of a derivation is the product of the probability of the fragments it uses. Note that a single tree can be the result of different derivations. Therefore probability of a tree is the sum of the probabilities of all its derivations.

\paragraph{Theoretical issues}
It has been shown that DOP (in its original formulation) is biased and inconsistent \cite{johnson2002}, which are both assumed to be bad properties of an estimator in general. As we will see, bias is not necessarily a bad thing. In fact, Zollman proves in  \cite{zollmann2005} that any non-overfitting estimator is biased. Furthermore, he shows that it is possible to define a DOP-estimator that is provably consistent.


\paragraph{Practical issues}
In its original formulation, DOP takes the trees apart in all possible ways. The number of fragments is exponential in the length of the sentences, thus the size of the symbolic grammar would be far too huge to be computationally feasible. 
Different approaches have been taken to reduce the symbolic grammar, e.g. by sampling or by applying a smart algorithm.

\subsection{Our contribution}

The main question of this work concerns the correspondence between theoretical properties and practical performance. Is it possible to gain theoretically attractive properties without performance losses? Can we reduce overfitting in an already well-performing approach by introducing the right bias? How do consistent grammars and well-performing grammars differ in their fragment weight distribution?

We compare two approaches that aim to solve the issues we pointed out: \ddop{}\cite{sangati2011} and \dops{}\cite{zollmann 2005}. 
We trained three grammars related to these approaches on part of the WSJ treebank. We hope the comparison of grammars leads to profound insight into the internal distribution of the model, in order to elucidate the correspondence between the fragment weights and theoretical properties.

\subsection{Outlook}
Section~\ref{sec:Statistics} elaborates the notions of consistency and bias and their relation to overfitting.
In section~\ref{sec:Existing}, we dive a detailed description of the estimators \ddop{} and \dops{}.
Section~\ref{sec:Comparison} offers a detailed comparison of the two as well as a description of the experiments we conduct. 
We investigate the differences between the grammars produced by \ddop{} \dops{}. The algorithms can be decomposed into two parts. We also analyze the impact of the partial choices by mutually using these parts.

In section~\ref{sec:Results}, we present our findings and provide an analysis. 


