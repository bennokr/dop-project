\section{Statistics: Bias and Consistency}\label{sec:Statistics}



% Distribution, sampling, estimation, corpus.
Linguistic studies of syntax mostly concern \emph{competence} models, which describe the which structures appear in a language. In contrast, a \emph{performance} model of language is an estimate of the probability of observing a parse tree in language use. It treats language as a statistical distribution over syntactic structures.

Let $\Omega$ be the set of all possible parse trees. The distribution $P_\Omega$ then describes the language, where $P_\Omega(t)$ is the probability of observing tree $t \in \Omega$. Using a sample of parse trees from the language, an \emph{estimator} $\varphi$ builds a statistical model. A parser then uses that statistical model to predict the correct parse tree of sentences.
A sample $X \in \Omega^n$ from the language is called a \emph{corpus} or \emph{treebank} of size $n$. If $\mathcal{M}$ is the set of probability distributions over $\Omega$,

\begin{align}
P_\Omega &\in \mathcal{M} \\
\varphi(X) &\in \mathcal{M}
\end{align}

% Expectation given a distribution, Loss, risk, mean squared difference, error.
In theory, an estimator should make exactly the right estimations of probabilities if it's given an infinite amount of data. That is to say, it should \emph{converge} to the true distribution. If an estimator converges in the limit, that estimator is \emph{consistent}.
However, given a finite amount of data, the estimator will probably not generate the correct probabilities. The distance between the true distribution and an estimate is called the \emph{loss} of that estimate. The loss can be defined in different ways, but the most popular is the \emph{mean squared difference}:

$$ \mathcal{L} $$

From a true distribution, it's possible to calculate the expected loss of an estimator trained on a treebank of a certain size. This is the \emph{risk} of that estimator given a sample size and a distribution.

$$ \mathcal{E} $$

%Other estimators:

%\emph{relative frequency}

%EWE, MLE



%- Why DOP*?



%- How do we prevent overfitting?

Bias is good.
