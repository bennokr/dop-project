
\section{Comparison} \label{sec:Comparison}
%The theoretical properties of \dops{} and \ddop{} differ:  
\dops{} and \ddop{} differ both in the set of fragments they extract and their estimation of the weights. To investigate the exact differences, we will view both steps separately.

%Note that the \dops{} extraction needs another decision: in many cases, there are several shortest derivations possible. From now on, we add all fragments that occur in one of these shortest derivations to the symbolic grammar. Of course we need to adjust the weights (e.g. divide the frequency counts by the number of shortest derivations) so that no full tree gets a higher impact on the PTSG. We will keep to the original formulation of \dops{} in case no derivation is possible, i.e. not including any fragments for this tree.

\paragraph{Extraction}
\ddop{} uses a tree kernel approach to find the maximal overlapping fragments of pairs of trees, which are added to the symbolic grammar. We will call this the \emph{maximal-overlap} method. \dops{} iteratively finds the shortest derivation of one tree given all the fragments of a set of trees, hereafter the \emph{shortest-derivation} method. 

It is easy to see that the \emph{shortest-derivation} extraction in itself does not depend on the corpus split: we can also find the shortest possible derivation using fragments from all the other trees. Likewise \ddop{} could be implemented using a split, comparing all trees in the $HC$ to all trees in the $EC$.
We will refer to these methods as \emph{one vs. rest} and \emph{split} estimation.

%Whether the corpus is split in two does only influence the size of the symbolic grammar and

%Therefore, w
%We will implement both extraction methods in a 1 vs the rest manner. In this way, we can analyse how the resulting symbolic grammars differ. The analysis will comprise the size of the resulting symbolic grammar and the relative number of fragments of certain depth. Furthermore, we might be able to find interesting patterns by manually looking at the fragments that were extracted by one of the systems only.

\paragraph{Estimation}

Both approaches use the relative frequencies of the fragments for the weights:
\begin{align}p(f)=\frac{count(f)}{\sum_{f'\in F_{root}(f)} count(f')}\end{align} 
However, in the \ddop{} case these values refer to exact counts of the fragments in the treebank, whereas in \dops{} they refer to occurrence of fragments in shortest derivations.


\ddop{} determines the weights of the fragments in the symbolic grammar in a separate run over the treebank, to obtain exact counts. We use the relative frequency estimate to assign weights to the fragments. \dops{} on the other hand counts the occurrence in shortest derivations of the fragments, and normalizes relative to counts of fragments with the same root.

To maximize coverage of the grammar, both \ddop{} and \dops{} apply smoothing that slightly alters the weights. Next to extracting maximally overlapping fragments, \ddop{} extracts all CFG rules and estimates their weight. \dops{} is smoothed by calculating the weight of the unparsed sentences, and distributes this over the CFG rules.

 %Furthermore, infrequent words are replaced by a lexical description.

%HIER



\paragraph{Example}
\FloatBarrier
This example clarifies how the grammars that result from \ddop{} and \dops{} can actually differ. Recall our toy treebank from figure \ref{f:treebank} and the fragments in figure \ref{f:fragments}. 
Applying the maximal overlap extraction and shortest derivation extraction in a 1 vs the rest manner to this treebank, yields the weights in table \ref{t:weights}.

Note the remarkable differences in the weight distributions. For example, $f_1$ gets a weight of 0.5 in the maximal overlap approach, and zero in the shortest derivation case. Of course, the sparsity of the data contributes much to these extreme variations. However, the observed differences encourage us to investigate these two approaches into more depth.



\begin{table}[h!]
\center
\input{tableWeights}
\caption{The weights assignment according to both methods in a one vs. the rest manner}
\label{t:weights}
\end{table}
\footnotetext{For this dataset, two shortest derivations exist for each tree. We refer to them with the following variables: 1a = f5, f6; 1b = f2, f10; 2a = f2, f8; 2b = f3, f6; 3a = f4, f8; 3b = f3, f7; 4a = f5, f7; 4b = f4, f9}





\subsection{Experiments}
%Description of our implementation, how does it resemble Double-DOP and DOP*? What are the differences? What are the theoretical capabilities? 





We compare the maximal overlap and shortest derivation extraction by using either a split or the whole set of trees for both estimators. We will plot the fragments according to the weights assigned by the estimators, such that the differences can stand out. In the same way, we compare the split and one vs. the rest estimation for the same estimator.

Furthermore, we can compare the grammars by having them parse a test set and determine their performance, e.g. the F1-score for correctly predicted parses.

\paragraph{Data}
We use the \emph{Wall Street Journal}~(WSJ) section of the Penn Treebank for our experiments. \dops{} has only been applied to the Dutch OVIS corpus in \cite{zollmann2005}, which contains relatively small and (therefore) easy sentences. Therefore we are curious about its performance on the WSJ.

The corpus was preprocessed by removing functions and binarizing the trees by Markovization (h=1, v=1). 

\paragraph{Algorithm}
First of all, we find the maximally overlapping fragments of all trees in the corpus, which corresponds to the \ddop{} approach.
Then, we randomly split the corpus ten times in two equally-sized parts, called $EC$ and $HC$. For each split, build a DOP-reduction grammar from $EC$ and use it to find the shortest derivations of the trees in $HC$, which corresponds to \dops{}. Additionally, we find the maximally overlapping fragments and estimate their weights from the $EC$. We'll call this \emph{Split Double-DOP}.

The results for the different splits were interpolated and the resulting grammars were smoothed as described above. For finding the weight of unparsable sentences in the \dops{} estimation, we needed to find the trees that had not been derived in any split. This was done by comparing each set of underived trees with the $HC$ sets of every split.

Estimation and parsing were done with the \tt{disco-dop} framework \ref{cranenburg?}.

\paragraph{Parsing}
The input for the parser consisted of sentences of word and POS tag pairs. The parser matches fragments to the whole pair when the word is known, but only uses the POS tag when the word is unknown.


% Estimating all grammars took 45 hours on 16 CPU cores.

\paragraph{Questions}
Comparing \dops{} and \ddop{}: Which estimator gives more weight to large fragments? Is this related to consistency?

Comparing \ddop{} and Split Double-DOP: What influence does a split have on performance and size of a grammar? How do the fragment weight distributions differ?

Comparing Split Double-DOP and \dops{}: Assuming that a larger weight in \dops{} corresponds to `usefulness' in parsing, what determines whether a fragment is useful?


%
%
%\paragraph{Extraction}
%Algorithm \ref{a:dops2} performs shortest-derivation extraction (as in \dops{}) in an efficient way, resembling the tree kernel approach in the iterative comparison of one tree with others. Namely, we iterate over the trees in $HC$ (lines 2-8): create a list $F$ of all of its fragments that occur in the rest of the treebank (line 3) and increment the count of the fragments in $F$ that occur in the shortest derivation(s). In this way, we do not need to store all fragments in $EC$ as in the initial step of the original \dops{} algorithm. 
%
%Algorithm \ref{a:ddop1} performs the maximal-overlap extraction (as in \ddop{}). For the comparison of trees to find the maximal overlap (line 4), the tree kernel approach from \cite{sangati2011} is used. 
%
%\begin{algorithm}
%{\bf Data}: a treebank $TB$\\
%{\bf Result}: a map of tree fragments and corresponding counts in shortest derivations\\
%\begin{algorithmic}[1]
%\State{Initialize $M$, a map from fragments to counts, empty}
%\For {$t\in TB$}
%	\State $F \gets Frag(t)\cap \bigcup\limits_{t'\in TB/\{t\}} frag(t')$
%	\State{$D\gets$ the shortest derivation(s) of $t$ using fragments in $F$}		
%	\For{$f\in D$}
%		\State add $f$ to $M$ {\bf if} $f\not\in M$
%		\State $M[f]\gets M[f]+1$ \Comment{Anticipating the estimation step}
%	\EndFor
%\EndFor
%%\For{$n \in V_N$}
%%	\State{Add the counts of all fragments rooted at $n$}
%%	\State{Normalize the weights of these fragments}
%%\EndFor	
%\end{algorithmic}
%\caption{Shortest derivation extraction in a one vs the rest manner}
%\label{a:dops2}
%\end{algorithm}
%
%
%\begin{algorithm}
%{\bf Data}: a treebank $TB$\\
%{\bf Result}: a set of tree fragments\\
%\begin{algorithmic}[1]
%\State{Initialize $M$, a map from fragments to weights, empty}
%\For{$t\in TB$}
%	\For{$t'\in TB/\{t\}$}
%		\State $f\gets$ maximal overlapping fragment(s) of $t$ and $t'$
%		\State add $f$ to $M$ {\bf if} $f\not\in M$
%	\EndFor
%\EndFor
%\end{algorithmic}
%
%
%
%\caption{Maximal overlap extraction in a one vs the rest manner}
%\label{a:ddop1}
%\end{algorithm}
%
%\paragraph{Estimation}
%Now for estimation, we use the output of the extraction algorithm. In the case of shortest-derivation extraction (algorithm \ref{a:dops2}), we only need to normalize the counts in $M$ to their relative frequency as compared to fragments that have the same root. In the case of maximal-overlap extraction (algorithm \ref{a:ddop1}), we need to iterate over the dataset once more to count the occurences of the fragments in the output and then compute their relative frequencies.
%
%%In \ddop, we would iterate over pairs of trees. With treebank size $n$, that would require $n!$%NEEN klopt niet! comparisons. In our proposed algorithm, we need to iterate over each combination of a tree in $HC$ with (worst case\footnote{The condition in line $5$ avoids unnecessary search for fragments. We might be able to pose a smarter constraint here, such that we only continue if we can possibly find fragments that lead to a shorter derivation}) all trees in $EC$. This requires $n/2 \times n/2$ comparisons. Furthermore, we immediately store the counts, so we don't need to iterate over the data again to assess the weights. We do however need to normalize the weights, such that the sum of weights of all fragment sharing the same root equals 1.




%In words: Iterate over the trees in $HC$ (lines 2-17): create a list $F$ of all of its fragments that occur in $EC$ (lines 3-12). This procedure resembles the treekernels from \ddop. Create the shortest derivation $D$ from fragments in $F$ (NB: this is not always possible, in which case $D=\emptyset$). Increment the counts of these fragments in the map $M$

%\begin{algorithm}[H]
%{\bf Data}: non-overlapping treebanks $EC$ and $HC$\\
%{\bf Result}: a PTSG, i.e. a map of tree fragments and corresponding weights\\
%\begin{algorithmic}[1]
%\State{Initialize $M$, a map from fragments to weights, empty}
%\For {$t_H\in HC$}
%%	\State $Q\gets frag(t_H)$
%%	\State $F \gets \emptyset$
%%	\While{$Q$ not empty}% or a smarter constraint
%%		\For{$t_E \in EC$}
%%			\For{$f \in Q\cap frag(t_E)$}
%%				\State $Q\gets Q/\{f\}$	
%%				\State $F\gets F\cup\{f\}$	
%%			\EndFor
%%		\EndFor
%%	\EndWhile
%	\State $F \gets Frag(t_H)\cap \bigcup\limits_{t_E\in EC} frag(t_E)$
%	\State{$D\gets$ the shortest derivation of $t_H$ using fragments in $F$}		
%	\For{$f\in D$}
%		\State add $f$ to $M$ {\bf if} $f\not\in M$
%		\State $M[f]\gets M[f]+1$
%	\EndFor
%\EndFor
%\For{$n \in V_N$}
%	\State{Add the counts of all fragments rooted at $n$}
%	\State{Normalize the weights of these fragments}
%\EndFor	
%\end{algorithmic}
%\caption{\dops{} extraction and estimation}
%\label{a:dops1}
%\end{algorithm}