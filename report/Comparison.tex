
\section{Comparison} \label{sec:Comparison}
%The theoretical properties of \dops{} and \ddop{} differ:  
\dops{} and \ddop{} differ both in the set of fragments they extract and their estimation of the weights. To investigate the exact differences, we will view both steps separately.

\paragraph{Extraction}

\ddop{} iterates over pairs of trees to find their maximal overlapping fragments, which are added to the symbolic grammar. We will call this the \emph{maximal-overlap} method. \dops{} iteratively finds the shortest derivation of one tree given all the fragments of a set of trees, hereafter the \emph{shortest-derivation} method. 

It is easy to see that the \emph{shortest-derivation} extraction in itself does not depend on the corpus split: we can also find the shortest possible derivation using fragments from all the other trees. Likewise \ddop{} could be implemented using a split, comparing all trees in the $HC$ to all trees in the $EC$.
We will refer to these methods as \emph{full} and \emph{split} estimation.

\paragraph{Estimation}

Both approaches use the relative frequencies of the fragments for the weights:
\begin{align}p(f)=\frac{count(f)}{\sum_{f'\in F_{root}(f)} count(f')}\end{align} 
Where $F_{root}(f)$ denotes the fragments in the grammar that have the same root as $f$.
However, in the \ddop{} case these values refer to exact counts of the fragments in the treebank, whereas in \dops{} they refer to occurrence of fragments in shortest derivations.

\paragraph{Smoothing}
To maximize coverage of the grammar, both \ddop{} and \dops{} add PCFG rules to the grammar. In \ddop{} this is done before the estimation, such that the CFG rules are treated as if they were extracted as maximal overlapping fragments.
\dops{} is smoothed by calculating the weight of the unparsed sentences: $p_{unkn}$, and distributes this probability mass uniformly over the treebank PCFG grammar.




\paragraph{Example}
\FloatBarrier
This example clarifies how the grammars that result from \ddop{} and \dops{} can actually differ. Recall our toy treebank from figure \ref{f:treebank} and the fragments in figure \ref{f:fragments}. 
Applying the maximal overlap extraction and shortest derivation extraction with full estimation to this treebank, yields the weights in table \ref{t:weights}.

Note the remarkable differences in the weight distributions. For example, $f_1$ gets a weight of $p_k\times 0.5$ in the maximal overlap approach, and zero in the shortest derivation case. This is not a fair comparison of course, the sparsity of the data contributes much to these extreme variations. However, the observed differences encourage us to investigate these two approaches in more depth.


\begin{table*}[t]
\center
\input{tableWeights}
\caption{The weights assignment according to both extraction methods in a full estimation manner.\\
{\footnotesize$^*$ The fragment occurs in the maximal overlap of these pairs of trees\\
$^{**}$ For this dataset, two shortest derivations exist for each tree:
$t_1=f_5\circ f_6$~(1a)~or~$t_1=f_2\circ f_{10}$~(1b), 	$t_2=f_2\circ f_8$~(2a)~or~$t_2=f_3\circ f_6$~(2b),
$t_3=f_4\circ f_8$~(3a)~or~$t_3=f_3\circ f_7$~(3b),	$t_4=f_5\circ f_7$~(4a)~or~$t_4=f_4\circ f9$~(4b)\\
$^{***}$ Smoothing: $p_u$ is short for $p_{unkn}$, $p_k = 1- p_{unkn}$
}
}
\label{t:weights}
\end{table*}




\subsection{Experiments}
%Description of our implementation, how does it resemble Double-DOP and DOP*? What are the differences? What are the theoretical capabilities? 

We compare the original formulation of \ddop{} and \dops{}, i.e. maximal overlap with full estimation and shortest derivation with split estimation. Furthermore, we add a new estimator that is a hybrid of the two: maximal overlap with split estimation. We have not been able to provide an efficient  implementation of shortest derivation extraction with full estimation.


Estimation and parsing were done with the {\tt disco-dop} framework
 \footnote{http://github.com/andreasvc/disco-dop, \cite{cranenburgh2011}}.


\paragraph{Data}
We use the \emph{Wall Street Journal}~(WSJ) section of the Penn Treebank for our experiments. \dops{} has only been applied to the Dutch OVIS corpus in \cite{zollmann2005}, which contains relatively small and (therefore) easy sentences. Therefore we are curious about its performance on the WSJ.

The commonly used training section of the WSJ treebank (section 2-21) was preprocessed in {\tt disco-dop} by removing functions and binarizing the trees by Markovization (h=1, v=1). %\footcite{klein2003}. 
%UITLEG/ CITATIE



\paragraph{Algorithm}
First, we find the maximally overlapping fragments of all trees in the corpus, which corresponds to \ddop, and we call the \emph{Full Maximum Overlap} approach.
Then, we randomly split the corpus ten times in two equally-sized parts, called $EC$ and $HC$. For each split, build a DOP-reduction grammar from $EC$ 
%HIER IETS TOEVOEGEN
Use this grammar to find the 1000 best derivations of trees in $HC$. We make the simplifying assumption that the shortest derivations in this set  represent the true set of shortest derivations: \emph{Split Shortest Derivations}.

 Additionally, we find the maximally overlapping fragments of pairs consisting of a tree from $HC$ and one from $EC$. The weights are estimated from the $EC$. We'll call this \emph{Split Maximum Overlap}.

The results for the different splits were interpolated as follows: for each fragment, its weight is defined as the arithmetic mean of the weights from the 10 grammars. The weight is assumed to be zero for grammars that do not contain some fragment.
The resulting grammars were smoothed as described above. For finding $p_{unkn}$, the weight of unparsable sentences in the \dops{} estimation, we needed to find the trees that had not been derived in any split. This was done by comparing each set of underived trees with the $HC$ sets of every split.

\paragraph{Parsing}
We used the development set of the WSJ treebank, section 24. The input for the parser consisted of sentences with a POS-tag attached to each word. The parser matches fragments to the whole pair (word and POS-tag) when the word is known, but only uses the POS tag in case the word is unknown. Again, the 1000 best derivations were used to approximate the most probable parse tree.


% Estimating all grammars took 45 hours on 16 CPU cores.

% \paragraph{Questions}
% Comparing \dops{} and \ddop{}: Which estimator gives more weight to large fragments? Is this related to consistency?

% Comparing \ddop{} and Split Double-DOP: What influence does a split have on performance and size of a grammar? How do the fragment weight distributions differ?

% Comparing Split Double-DOP and \dops{}: Assuming that a larger weight in \dops{} corresponds to `usefulness' in parsing, what determines whether a fragment is useful?

